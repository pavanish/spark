{"cells":[{"cell_type":"markdown","source":["## Pavanish Kumar, PhD\nContact me __[linkedIn](https://www.linkedin.com/in/kumar-pavanish-5340a224/)__"],"metadata":{}},{"cell_type":"markdown","source":["#### This tutorial aims to demostrate some of the spark/pyspark ml APIs.\n\nWith the spark 2.x release, newer machine learning module was also released which is refred as spark.ml. The previous library spark.Mllib was built on RDD api, however spark.ml is built on dataframe api and thus its api is cleaner,faster and easier to implement. The best thing about using spark for machine learning is that the same code you wrote for small data set can be scalled to massive size dataset. No extra effort is required to build parrallel implementation, everything is abstracted in spark framework. So you can build the modal on your labtop in local spark mode (better alternative is to use Databrics community eddition cluster) using sample dataset, and deploy the same code on large spark cluster."],"metadata":{}},{"cell_type":"markdown","source":["#### pyspark ml have api for most commenly used machine learning algorithms eg:\n- Linear regression\n- Logistic regression\n- Collaborative filtering (ALS)\n- K-means\n- PCA\n- Decision tree classifier\n- Random forest classifier\n- Gradient-boosted tree classifier\n- Multilayer perceptron classifier\n- Linear Support Vector Machine\n- and more..."],"metadata":{}},{"cell_type":"markdown","source":["The Tutorial is organised in Following section:\n  1. - Data description\n  2. - Machine Learning Workflow\n          \n  4. - Improving prediction model: Parameter tunning\n  5. - Extracting the best prediction model"],"metadata":{}},{"cell_type":"markdown","source":["#### 1.Data Description\n\nFor this demo I will use forest cover data from uci machine learning repository\n\n __Context__ <br>\nThis dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables from 30 meter x 30 meter sections of forest. \n\n__Content__ <br>\nThe dataset includes information on tree type, shadow coverage, distance to nearby landmarks (roads etcetera), soil type, and local topography.\noriginal data was oneHotEncoded for wilderness_Area and soil_type\n\nOriginal data had seperate column for each soil_type from Soil_Type1 to Soil_Type40 with value  0 or 1 \nsimillary Wilderness_Area was also had four columns Wilderness_Area1 to Wilderness_Area4 with value 0 or 1\n\nIn this format data takes more storage space and not an efficient way to store the data, so i have transformed the data back to one variable each for soil_type(with 40 levels) and wilderness area(with 4 levels). Ttransformed data has 12 columns, including last true label column i.e Cover_Type. Transformed data can be found here : https://github.com/pavanish/spark/tree/master/data\n\n\n__The task here is to a build prediction model on 11 features to predict the cover type__"],"metadata":{}},{"cell_type":"markdown","source":["#### 2. Machine Learning workflow\nTo build a prediction modal we need:\n  > 1. Prediction algorithm\n  > 2. Training data (If its a classification problem)\n  > 3. Build the model\n  > 4. Model evaluation\n\nSo, for this dataset lets try Logistic regression and decision tree for prediction model.\nTo access these algorithms lets import these from pyspark"],"metadata":{}},{"cell_type":"markdown","source":["#### _Machine Learning workflow_ :\n> - Prediction algorithms\n\nWe first need to choose an appropriate algorthms to build the model and import it from the pyspark ml module.\nAs our task is to classify the multiclass cover type, following two simple classification algorthms can be tried\n> Logistic Regression <br>\n> decision tree\n\nSo let`s import these algorthims first"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["#### Lets read and explore the data first\nspark 2.x has inbuilt API for reading CSV file \nwhich can be accessed from spark session available as spark"],"metadata":{}},{"cell_type":"code","source":["# read the csv file\n# inferSchema if set to true will infer the data type by sampling the data, schema can be given in .option()\n# inferSchema is less efficient as it need to read and sample the data to get the data type, for real big data in real application providing schema using .option() method will be a better choice\n# as the first line of the data is header lets set header to True\n\nforest_cover=spark.read.csv(\"/FileStore/tables/cover_type_uncode_csv\",\n                            inferSchema=True,header=True)\n\n# spark.read.csv returns the data in dataframe"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["__Persisting the data in memory__ <br> \nif we going to refer data many time its prudent to cache it in the memory for faster access.\nmachine learning algorithms uses iterative update so caching the data will run the application faster and efficiently"],"metadata":{}},{"cell_type":"code","source":["# at this step it will not cache the data, well spark is lazy enough to not act on your command unless you really force it to enact using either of the action methods\n# df.cache is transformation and because it`s lazy evalution it does nothing.\n\nforest_cover.cache()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# to be sure that data is cached, chain some action to it eg: .count() method which is action method\nforest_cover.cache().count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# lets dive in and see data-structure using df.show(n),  n => number of rows to display. By default; n=20\nforest_cover.show(5)\n\n# there is 13 columns in total, first column _c0 is useless and it came because i transformed and saved the data using R and forgot to set row.names=FALSE. Anyway, its an opportunity to discuss one more method => df.drop('colname')\n"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["forest_cover.columns # get the names of the columns"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["forest_cover = forest_cover.drop('_c0') # lets drop the artifact '_c0' column using df.drop() method"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["forest_cover.columns # sure enough the _c0 gone now"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["forest_cover.count() # let see total observations count"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["coverType is the label which we need to predict, so let`s see how many categories are for forest_cover in the label variable cover_type. <br>\ngroupBy method on dataframe can be called to group on the cover_type variable and then sort and count"],"metadata":{}},{"cell_type":"code","source":["forest_cover.groupBy('coverType').count().sort('count',ascending=False).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Its time to check datatype using df.printSchema()\n# for evry column it shows colname:dataType(nullable=true) \n# eg;Elevation: integer (nullable = true)\n# datatype for the soil is integer as soil type is coded as 4 digit number, we need to cahnge it to stringType\n# Will also change the coverType from integer to stringType \nforest_cover.printSchema()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# we can change the datatype using df.withColumn() api\n# df.withColum takes two parameter 1: name and 2.column and append new transformed column to the dataframe\n# forest_cover=forest_cover.withColumn('label',forest_cover.coverType.cast('String')) ## here the 'label' is the name of the column and column is refered as df.colname and it is chained with another transformation .cast(). \nforest_cover=forest_cover.withColumn('label',forest_cover.coverType.cast('String'))\nforest_cover=forest_cover.withColumn('soil',forest_cover.soil.cast('String'))\n\n# as we have coverType as label lets drop this coverType from the dataframe\nforest_cover = forest_cover.drop('coverType')\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# lets check the schema again\nforest_cover.printSchema()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["#### _Machine Learning workflow_ :\n> - Get training data\n\nSo far we imported the pyspark functions and prediction algorthms from pyspark ml api, read,explored and transformed the data <br>\nNow lets go to step 2 of the Machine Learning workflow and get the training data for training the prediction model<br>\npysprk provides nice easy function to split the data randomly in desired proportion. We will split the original data into 80,20 ratio <br>\n80 % for training, 20 % for test"],"metadata":{}},{"cell_type":"code","source":["splits = forest_cover.randomSplit([0.8, 0.2])  # df.randomSplit() takes list of proportion as input\ntrain = splits[0] # splits is list with datasets, as we split it into two first 0.8 (80 %) set it to train\ntest = splits[1]  # 0.2 (20%) test data\n\n# spliting is random and approximate\n# lets see how many observations are ther in each train and test data set\ntrain_rows = train.count()\ntest_rows = test.count()\nprint \"Training Rows:\", train_rows, \" Testing Rows:\", test_rows"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Now the data is ready, let`s move on to assemble the components of prediction modal and implement it. <br>\npyspark provide various method required for buliding prediction modal pipeline.\nInfact along with various method it also provide a pipeline API which assembls each components of the pipeline and thus ensures the data flow through the pipeline in a consistent manner.The pyspark api provides consistent api across the different algorithms which makes it really easy to test and try many diffrent ml algorithms with a small change in code. It gives the plug and play modular framework. <br>\n\neach ml algorithms expect feature vector as input for traning the model, and __VectorAssembler__ does just that. It takes the input column in a list and returns the FeatureVector. <br>\nwe also need to transform the categorical feature using __stringIndexer__ method which take categorical feature as input column and return indexed feature.\nwe will use stringIndexer to index the soil and wilderness column. We need to indexed the label as well.\n\n\nthe data has two columns of categorical features; soil(40 categories) and wilderness(4 categories)"],"metadata":{}},{"cell_type":"markdown","source":["#### _Machine Learning workflow_\n> - Build the model\n\nTo build the prediction model few components are needed, let`s export these from pyspark ml. <br>\nFollowing are components needed to build the model : \n  > __pipeline__, __VectorAssembler__, __StringIndexer__, __VectorIndexer__<br>\n\n  __pipeline__ => It String toghether the stages of transformation, in this case pipeline has following stages : stages=[labelIndexer,strIdx1,strIdx2, catVect,catIdx, numVect, featVect, lr] <br>\n  __stringIndexer__ => It indexex the categorical or string type into numeric type. <br>\n  __VectorIndexer__ => VectorIndexer index the categorical feature, it does oneHotEncoding whcih means it will spread the soil 40 categories in vector of 40 feature <br>\n  __VectorAssembler__ => it takes the feature columns as list and return the feature vector\n  \n  ##### Stages in our model building to predict coverType\n  > labelIndexer = this satge will index the label into numericType # <br>\n  > strIdx1, and strIndx2 = it indexes the categorical feature <br>\n  > catIdx = VectorIndexer index the categorical feature, and does oneHotEncoding <br>\n  > numVect = numeric data was assembled as vector using vector assembler <br>\n  > featVect = both the categorical and numeric feature are tied toghether into single feature vector again using VectorAssembler <br>\n  > lr/dt = gets the algorhitum from pyspark, algorithm specific model parameter can be specified here <br>\n  > lr_pipeline = At the end we pass the stages of transformation to pipeline method, pipeline ensures train and test data flow though the transformation in a consistent manner <br>\n\npipeline returns the __estimator__ and ones we fit the model with train data it returns the __transformer__ <br>\n\nlr_pipeline/dt_pipeline is an __estimator__"],"metadata":{}},{"cell_type":"code","source":["\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer, MinMaxScaler\nfrom pyspark.ml import Pipeline"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["\nlabelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\")\nstrIdx1 = StringIndexer(inputCol = \"wilderness\", outputCol = \"waIdx\")\nstrIdx2 = StringIndexer(inputCol = \"soil\", outputCol = \"soilIdx\")\ncatVect = VectorAssembler(inputCols = [\"waIdx\",\"soilIdx\"], outputCol=\"catFeatures\")\ncatIdx = VectorIndexer(inputCol = catVect.getOutputCol(), outputCol = \"idxCatFeatures\")\nnumVect = VectorAssembler(inputCols = [\"Elevation\",\"Aspect\",\"Slope\",\n                                       \"Horizontal_Distance_To_Hydrology\",\n                                       \"Vertical_Distance_To_Hydrology\",\n                                       \"Horizontal_Distance_To_Roadways\",\n                                       \"Hillshade_9am\",\n                                       \"Hillshade_Noon\",\n                                       \"Horizontal_Distance_To_Fire_Points\"], outputCol=\"numFeatures\")\n#minMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normFeatures\") \nfeatVect = VectorAssembler(inputCols=[\"idxCatFeatures\", \"numFeatures\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\",maxIter=10,regParam=0.3)\ndt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\",maxBins= 50)\nlr_pipeline = Pipeline(stages=[labelIndexer,strIdx1,strIdx2, catVect,catIdx, numVect, featVect, lr])\ndt_pipeline = Pipeline(stages=[labelIndexer,strIdx1,strIdx2, catVect,catIdx, numVect, featVect, dt])\n\n# you can see how easy it is to use 2 machine learning method, we just change last stage of the pipeline form lr to dt"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# lets fit decision tree\n\ndtModel = dt_pipeline.fit(train)\nprint \"decision tree Pipeline complete!\""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# fit logistic regression modal\n# after we use the .fit method it returns transformer so lrModel is transformer\nlrModel = lr_pipeline.fit(train) \nprint \"logistic regression Pipeline complete!\""],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# lets predict the test data using transform method on model return after fiting the data\nlr_prediction = lrModel.transform(test)\ndt_prediction = dtModel.transform(test)\n\n# transform method will add additional column of prediction to test data set"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["#### _Machine Learning workflow_ : \n>  - Modal evaluation \n\nOnce the model is built and trained, we can use the model to predict the unseen data. \nTo evaluate how the prediction model perform we need an appropriate metric to assess the model performance.\nFor our example we can use accuracy as metric for model evaluation.\ntransform method on model add the prediction column to the test data which we can be used to calculate the accuracy. pyspark has some built in evulator metrics which can utilised to measure model performace.<br>\nAs the task was to predict one of the 7 label (coverType)(1,2,3,4,5,6,7) we need MulticlassClassificationEvaluator which we already imported in above code cells (Cmd 8) its takes labelCol, predictionCol and metricName as input and returns evaluator on which .evaluate method is applied to get the metric from the dataframe having required columns set by labelCol, predictionCol and metricName"],"metadata":{}},{"cell_type":"code","source":["evaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\ndt_accuracy = evaluator.evaluate(dt_prediction)\nlr_accuracy = evaluator.evaluate(lr_prediction)\nprint(\"Decision tree Test Error = %g \" % (1.0 - dt_accuracy))\nprint(\"Logistic regeression Test Error = %g \" % (1.0 - lr_accuracy))\nprint (\"Decision tree prediction accuracy = %g \" % dt_accuracy) \nprint (\"Logistic regeression prediction accuracy = %g \" % lr_accuracy)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Accuracy for logistic regression is around 59% while accuracy for decision tree is 10% higher at 69% <br>\nThis does not mean that logistic regression is bad algorithm probably parameter tunning is needed <br>\nIt only suggest that with default parameters decision tree works best for this dataset"],"metadata":{}},{"cell_type":"code","source":["# Confusion matrix can be created using crosstab function.\ndt_prediction.crosstab(\"indexedLabel\",\"prediction\").show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["#### Lets explore the prediction results in a bit more detail"],"metadata":{}},{"cell_type":"code","source":["## lets select only required columns i.e prediction and indexedLabel\n## for some explanation let also get features and label columns using df.select('colname') method\n## feature has 11 values (we had 11 feature columns in data set) assembled in vector using VectorAssembler\n## label is true label, indexedLabel is label transformed by stringIndexer use in labelIndexer stage : There is something important to discuss about stringIndexer hold on for a while\n\n#predicted = lr_prediction.select(\"features\", \"prediction\", \"indexedLabel\",\"label\")\npredicted = dt_prediction.select(\"features\", \"prediction\", \"indexedLabel\",\"label\")\npredicted.show(10, truncate=False)\npredicted.groupBy('prediction').count().show() \n# this just show few observation and it seems prediction was not good: just match prediction col to indexedLabel. in the intial 10 obsevation all 4 is predicted as 2 and 2 is predicted as 6 only last observation is predicted correctly # Note: there is randomness in the .show() method so if you run code again and again you may get different set of data. If we group the prediction column and observe it we find that label 4 is not predicted at all.\n"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["predicted.groupBy('label').count().show() # observe the real label distribution"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["predicted.groupBy('indexedLabel').count().show() ## this shows the fraction of data that belongs to each class of label\n\n# Now is the good time to discuss stringIndexer: "],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["#### StringIndexer\n\n**_StringIndexer_** takes the label with string type and map it with int type \nso in its conversion it maps the label according to their count order.\nStringIndexer orders the label accoring to the count and map the label from 0 to total number of labels\nwith 0 mapped to label with max count\n\neg, lets say our labels are of string type Apple, Banana, orange, and Mango\n with thier respective counts as 50, 500, 300, and 400\n\n\"Apple\":50,\"Banana\":500,\"Orange\":300,\"Mango\":400 : original labels with total counts\n\n\nStringIndexer will reorder it to \n**\"Banana\":500,\"Mango\":400,\"Orange\":300,\"Apple\":50 => (\"Banana\",\"Mango\",\"Orange\",\"Apple\")**\n\nand mapped to (\"Banana\":0,\"Mango\":1,\"Orange\":2,\"Apple\":3) \nfinally will have indexedLabel as => **(0,1,2,3)**\n\n\n\n#####in the forest cover data labels are cover_type labelled as 1,2,3,4,5,6,7\nyou can look at the frequency of these labels (Cmd 40) by prediction.groupBy(\"label\").count().show()\n\norder of the label \"2\",\"1\",\"3\",\"7\",\"6\",\"5\",\"4\"\n\nStringIndexer will map it to \"2\":0,\"1\":1,\"3\":2,\"7\":3,\"6\":4,\"5\":5,\"4\":6  so real label 2 is mapped to 0, 1 to 1 and 3 to 2 and so on..."],"metadata":{}},{"cell_type":"markdown","source":["#### 3. Improving prediction model: Parameter tunning\n The initial analysis suggested that decision tree might be a better prediction algorithum for this data. \n Let`s further fine tune the modal and evalute whether prediction accuracy improves.\n pyspark provide built in functionality for selecting the best mmodal and makes parameter tunning an easy task."],"metadata":{}},{"cell_type":"code","source":["# lets import api for parameter tunning\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Let`s tune the parameter for decision tree and find the best modal\n# ParamGridBuilder() api in spark makes it easier to tune the combination of parameter by making grid of parameter to train on\n# different parameter can be added to ParamGrid using model.addGrid option \n# as the model will be trained for various combination of parameter we need to pass Evaluator to assess the accuracy of prediction by modal\n# we will also use TrainValidationSplit method to do cross validation of the model \n\nparamGrid = ParamGridBuilder().addGrid(\n    dt.impurity, [\"gini\", \"entropy\"]).addGrid(\n    dt.maxDepth, [1, 20]).addGrid(\n    dt.maxBins, [40, 300]).addGrid(\n    dt.minInfoGain, [0.0, 0.05] ).build()\n\nMulticlassEval = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\",predictionCol=\"prediction\",metricName=\"accuracy\")\n\ntvs = TrainValidationSplit(estimator=dt_pipeline, evaluator=MulticlassEval, \n                           estimatorParamMaps=paramGrid, trainRatio=0.9) # pass it the ml pipeline, evualator and paramGrid and finaly trainRatio 0.9 means it will use 90% of data for training and 10% test for prediction accuracy\n                                              \n\nmodel = tvs.fit(train) # this will take a while to run as it`s training 16 model (total 4 parameter with 2 values each). On databrics community cluster it took around 9 minutes: so run it and take a break\n\n# model here will have the final best model which can be used to predict the new data"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["prediction = model.transform(test)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Lets see the accuracy after parameter tunning\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(prediction)\nprint(\"Test Error = %g \" % (1.0 - accuracy))\nprint (\"Decision tree prediction accuracy = %g \" % accuracy) "],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["## ooooo.... such a dramatic improvement in prediction accuracy\n## it jumps to 92% from 70%"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["#You can get the confusion matrix using crosstab function\nprediction.crosstab(\"indexedLabel\",\"prediction\").show()\n\n# prediction is along the row and true lable is along col\n# so with improved modal, class 5 (1916 observation) is correctly predicted as 5 in 1388 observation\n# with the initial naive modal, only 13 observation was correctly predicted as 5 "],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["#### 4. Extracting the best prediction model\nFinally we want to get the parameter of the best model selected by TrainValidationSplit class <br>\nTrainValidationSplit class resturns the evaluator and when fit method is used on it with training data it returns transformer <br>\nfinal transformer here is __model__ created by using fit method on tvs object (Cmd 45)<br>\n\nmodel.bestModel gives the bestModel evaluated on parameter grid, .stages on best model returns the list of stages. The classification model is the last element in the stage which can be assesed in python using [-1] subsetting"],"metadata":{}},{"cell_type":"code","source":["model.bestModel.stages"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["bestModel = model.bestModel.stages[-1]\nbestModel"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["for pyspark to work, Java TrainValidationSplitModel was needed as python object which is done by @classmethod def _from_java(cls, java_stage):<br>\nand the another @classmethod def _to_java(self): return a java object which can be used to access the parameters<br>\n\nThe detail code is here __[spark ml github](https://github.com/apache/spark/blob/master/python/pyspark/ml/tuning.py)__"],"metadata":{}},{"cell_type":"code","source":["#Lets get the parameter of the best trained model\nprint (bestModel._java_obj.getImpurity())\nprint(bestModel._java_obj.getMaxDepth())\nprint(bestModel._java_obj.getMinInfoGain())\nprint(bestModel._java_obj.getMaxBins())"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["\n  "],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["# this gives all the parameter grid that got evaluated. We trained on combination of 2 parameter each for impurity('gini','entropy'), maxDepth(0,20),MaxBins(40,300),minInfoGain(0,0.5)\n# 4 parameter 2 each give total of 16 combination which means 16 models were evaluated to choose the best modal \nmodel.extractParamMap()\n\n# model.extractPramMap give all the model and parameter grid trained in sequencial manner\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# model.validationMetrics gives the evaluation metric in a list\n# as 16 models were trained it return list of 16 values\n# to know which model worked best np.argmax method can be used which shows model 15 is the best model np.argmax gives 14 as python is 0 index based its actually 15th model \n\nimport numpy as np\n\nprint(np.argmax(model.validationMetrics))\nmodel.validationMetrics"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["For explanation about decision tree, logistic regression and its parameter you can look in wikipedia or read this excellent book __The Elements of \nStatistical Learning__ //web.stanford.edu/~hastie/ElemStatLearn/. The book is made freely available by its genrous authors __Trevor Hastie, Robert Tibshirani,Jerome FriedmanIts__. \n\nIf you are data scientist and new to spark then pickup this book : __Learning Spark__ - O'Reilly Media. Althogh its bit outdated and cover spark 1.x, its a good for fundamental. <br>\nFor Machine Learning in spark you can get this O'Reilly book : __Advanced Analytics with Spark, 2nd Edition__. However the book uses scala language. The tutorial is somewhat based on one of the example from this book."],"metadata":{}},{"cell_type":"markdown","source":["#### __Tell me and I forget. Teach me and I remember. Involve me and I learn__\n\n> _benjamin franklin_"],"metadata":{}}],"metadata":{"name":"ML_RF_CoverType","notebookId":1475976655518269},"nbformat":4,"nbformat_minor":0}
